{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d6593f9-1d2c-484b-a074-102d560e259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"KETI-AIR/ke-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "99ce89be-d3c6-4a68-a517-5cc066fc14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"C:/Users/asia/Desktop/파이널프로젝트/03_데이터전처리/순화표현모델 데이터_리라이팅완료.jsonl\"\n",
    "\n",
    "# 줄 단위 JSON 로딩\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "955a0f88-ed62-41c8-b56f-d864a0e7475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] input_text:\n",
      "rephrase politely: A: 오늘 지각할 뻔했다ㅋㅋ 버스 진짜 안 옴 B: 나도 거의 뛰어서 옴; A: 수학쌤 오늘 또 지옥 시간일 듯 B: 그 ㅆㅂ 늙은이 목소리만 들어도 현타 옴\n",
      "[0] target_text:\n",
      "B: 수학쌤 목소리 너무 지루하긴 해\n",
      "============================================================\n",
      "[1] input_text:\n",
      "rephrase politely: A: 급식 오늘 뭐냐 ? B: 아마 제육볶음이었을걸 ? A: 개좋다ㅋㅋ 밥 두 공기 각 B: 니 그 돼지같은 몸으로 또 퍼먹냐ㅋㅋ 역겹다 진짜\n",
      "[1] target_text:\n",
      "B: 먹성 진짜 좋다 너\n",
      "============================================================\n",
      "[2] input_text:\n",
      "rephrase politely: A: 걔 오늘 교복에 단추 다 풀고 왔더라 B: 걍 꾸미고 싶었나보지 A: 근데 선생님한테 안 걸린 게 신기함 B: 지 꼴에 패션리더래ㅋㅋ 토나온다 진심\n",
      "[2] target_text:\n",
      "B: 걔 패션에 관심 많아보이긴 해 ㅋㅋ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    context = \"\\n\".join(example[\"context\"][:-1])  # 마지막 발화 제외\n",
    "    input_text = f\"rephrase politely:\\n{context}\\n{example['context'][-1]}\"  # 마지막 발화\n",
    "    input_text = clean_text(input_text)\n",
    "    target_text = clean_text(example[\"output\"])\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "\n",
    "processed_data = [preprocess(ex) for ex in data]\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"[{i}] input_text:\\n{processed_data[i]['input_text']}\")\n",
    "    print(f\"[{i}] target_text:\\n{processed_data[i]['target_text']}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e917ec2-acb1-45b4-ad0b-217f927818d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] input_text:\n",
      "rephrase politely: A: 오늘 지각할 뻔했다ㅋㅋ 버스 진짜 안 옴 B: 나도 거의 뛰어서 옴; A: 수학쌤 오늘 또 지옥 시간일 듯 B: 그 ㅆㅂ 늙은이 목소리만 들어도 현타 옴 B: 그 ㅆㅂ 늙은이 목소리만 들어도 현타 옴\n",
      "[0] target_text:\n",
      "B: 수학쌤 목소리 너무 지루하긴 해\n",
      "============================================================\n",
      "[1] input_text:\n",
      "rephrase politely: A: 급식 오늘 뭐냐 ? B: 아마 제육볶음이었을걸 ? A: 개좋다ㅋㅋ 밥 두 공기 각 B: 니 그 돼지같은 몸으로 또 퍼먹냐ㅋㅋ 역겹다 진짜 B: 니 그 돼지같은 몸으로 또 퍼먹냐ㅋㅋ 역겹다 진짜\n",
      "[1] target_text:\n",
      "B: 먹성 진짜 좋다 너\n",
      "============================================================\n",
      "[2] input_text:\n",
      "rephrase politely: A: 걔 오늘 교복에 단추 다 풀고 왔더라 B: 걍 꾸미고 싶었나보지 A: 근데 선생님한테 안 걸린 게 신기함 B: 지 꼴에 패션리더래ㅋㅋ 토나온다 진심 B: 지 꼴에 패션리더래ㅋㅋ 토나온다 진심\n",
      "[2] target_text:\n",
      "B: 걔 패션에 관심 많아보이긴 해 ㅋㅋ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 1단계: 전체 데이터 전처리\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = re.sub(r\"(ㅋ){3,}\", \"ㅋㅋ\", text)\n",
    "    text = re.sub(r\"(ㅎ){3,}\", \"ㅎㅎ\", text)\n",
    "    text = re.sub(r\"(;){2,}\", \";\", text)\n",
    "    text = re.sub(r\"(\\.{2,})\", \"...\", text)\n",
    "    text = re.sub(r\"(!){2,}\", \"!!\", text)\n",
    "    text = re.sub(r\"(\\?){2,}\", \"??\", text)\n",
    "    text = re.sub(r\"([!?.,])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"([~❤💢💥💬])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess(example):\n",
    "    context = \"\\n\".join(example[\"context\"])\n",
    "    input_text = f\"rephrase politely:\\n{context}\\n{example['input']}\"\n",
    "    input_text = clean_text(input_text)\n",
    "    target_text = clean_text(example[\"output\"])\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "\n",
    "# 전처리 적용\n",
    "processed_data = [preprocess(ex) for ex in data]\n",
    "\n",
    "# 결과 확인\n",
    "for i in range(3):\n",
    "    print(f\"[{i}] input_text:\\n{processed_data[i]['input_text']}\")\n",
    "    print(f\"[{i}] target_text:\\n{processed_data[i]['target_text']}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cda20ba4-aa67-4d40-9069-c2b4a4a4faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RewriteDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text = item[\"input_text\"]\n",
    "        target_text = item[\"target_text\"]\n",
    "\n",
    "        # 토큰화\n",
    "        model_inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        # squeeze로 (1, N) → (N) 차원 축소\n",
    "        model_inputs = {k: v.squeeze() for k, v in model_inputs.items()}\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"].squeeze()\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "311159a3-39b1-4c7b-8605-a55d58e460a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 분할\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dataset 인스턴스 생성\n",
    "train_dataset = RewriteDataset(train_data, tokenizer)\n",
    "test_dataset = RewriteDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b6fa70e-840b-42b5-b350-c11467d7cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./ke-t5-rewrite-small\",   # 결과 저장 디렉토리\n",
    "    learning_rate=2e-5,                   # 학습률\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,                   # 체크포인트 최대 2개 저장\n",
    "    num_train_epochs=3,                   # 에폭 수\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    generation_num_beams=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",          # 이거 꼭 넣어야 load_best_model_at_end가 작동함\n",
    "    save_strategy=\"steps\",               # 저장과 평가 전략 일치시켜야 에러 안 남\n",
    "    save_steps=500,                        # 50 step마다 저장\n",
    "    eval_steps=500,                        # 50 step마다 평가\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=100\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82460d82-d14f-4253-92a3-8f1df68fedd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='4050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/4050 07:32 < 53:40, 1.10 it/s, Epoch 0.37/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 05:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "예측: 아예 마찬가지입니다 마찬가지입니다原 누군가ramprampramprampramp 차질 차질 물적 평균점수 평균점수rampramprampramprampramprampramp 평균점수rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp 평균점수\n",
      "정답: B: 옛날 말투 같아서 오히려 정겹지 않아?\n",
      "---\n",
      "\n",
      "예측: 아예 마찬가지입니다 마찬가지입니다原 김신회rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp\n",
      "정답: B: 열심히 준비했으니 좋은 결과 나올 거야!\n",
      "---\n",
      "\n",
      "예측: 아예 마찬가지입니다 마찬가지입니다 마찬가지입니다 봐주 누군가 마아라 물적 평균점수 평균점수창업팀rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp\n",
      "정답: B: 좀 폐쇄적인 곳도 있으니 분위기 확인해보는 게 좋겠다\n",
      "---\n",
      "\n",
      "예측: 아예 마찬가지입니다 마찬가지입니다 마찬가지다창업팀 elsewhere 평균점수 평균점수ramprampramprampramp 평균점수ramprampramp 평균점수ramprampramp 평균점수rampramp 평균점수rampramp 평균점수rampramp 평균점수rampramp 평균점수rampramp 평균점수rampramp 평균점수ramp 평균점수rampramp 평균점수ramp 평균점수ramp 평균점수rampramprampramprampramprampramp 평균점수rampramp 평균점수 평균점수 평균점수\n",
      "정답: B: 그래, 서로 예민했을 수도 있지. 나도 잘 풀고 싶어.\n",
      "---\n",
      "\n",
      "예측: 아예 마찬가지입니다 마찬가지입니다原 누군가 물적 평균점수rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp\n",
      "정답: B: 반찬 쪽은 좀 심심했어. 익숙한 맛이긴 했지\n",
      "---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value('string'), 'references': List(Value('string'))}\nFeature option 1: {'predictions': Value('string'), 'references': Value('string')},\nInput predictions: ['아예', '마찬가지입니다', '마찬가지입니다原', ..., '평균점수rampramprampramprampramprampramp', '평균점수rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp', '평균점수'],\nInput references: [['B:', '옛날', '말투', '같아서', '오히려', '정겹지', '않아?']]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ke-t5-rewrite-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ke-t5-rewrite-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1538\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1539\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1540\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1541\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1542\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1914\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1911\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1912\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1914\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1916\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2268\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys_for_eval)\n\u001b[0;32m   2269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:166\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys, metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3019\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3016\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3018\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3019\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[0;32m   3020\u001b[0m     eval_dataloader,\n\u001b[0;32m   3021\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3022\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[0;32m   3023\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[0;32m   3024\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3025\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[0;32m   3026\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[0;32m   3027\u001b[0m )\n\u001b[0;32m   3029\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3310\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3306\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3307\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3308\u001b[0m         )\n\u001b[0;32m   3309\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3310\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels))\n\u001b[0;32m   3311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3312\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[54], line 23\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_preds)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# BLEU는 토큰화 필요\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m bleu_metric\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m     24\u001b[0m     predictions\u001b[38;5;241m=\u001b[39m[pred\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m decoded_preds],\n\u001b[0;32m     25\u001b[0m     references\u001b[38;5;241m=\u001b[39m[[label\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m decoded_labels]\n\u001b[0;32m     26\u001b[0m )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# ROUGE-L 계산\u001b[39;00m\n\u001b[0;32m     29\u001b[0m rouge_result \u001b[38;5;241m=\u001b[39m rouge_metric\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m     30\u001b[0m     predictions\u001b[38;5;241m=\u001b[39mdecoded_preds,\n\u001b[0;32m     31\u001b[0m     references\u001b[38;5;241m=\u001b[39mdecoded_labels,\n\u001b[0;32m     32\u001b[0m     use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:514\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m batch \u001b[38;5;241m=\u001b[39m {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_batch(batch)\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:596\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    595\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:616\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_example\u001b[1;34m(self, example)\u001b[0m\n\u001b[0;32m    609\u001b[0m feature_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature option \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures)])\n\u001b[0;32m    610\u001b[0m error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature_strings\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    615\u001b[0m )\n\u001b[1;32m--> 616\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value('string'), 'references': List(Value('string'))}\nFeature option 1: {'predictions': Value('string'), 'references': Value('string')},\nInput predictions: ['아예', '마찬가지입니다', '마찬가지입니다原', ..., '평균점수rampramprampramprampramprampramp', '평균점수rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp', '평균점수'],\nInput references: [['B:', '옛날', '말투', '같아서', '오히려', '정겹지', '않아?']]"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./ke-t5-rewrite-small\")\n",
    "tokenizer.save_pretrained(\"./ke-t5-rewrite-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "640617d7-7836-448a-93fb-60250bc02dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def load_model_and_tokenizer(model_path=\"./ke-t5-rewrite-small\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_bad_words(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def get_bad_words_ids(bad_words, tokenizer):\n",
    "    return tokenizer(bad_words, add_special_tokens=False).input_ids\n",
    "\n",
    "def rewrite(context, input_text, model, tokenizer, bad_words_file=\"badwords.txt\"):\n",
    "    # 프롬프트 생성\n",
    "    context_text = \"\\n\".join(context)\n",
    "    prompt = f\"rephrase politely:\\n{context_text}\\n{input_text}\"\n",
    "\n",
    "    # 비속어 로딩 및 ID 변환\n",
    "    bad_words = load_bad_words(bad_words_file)\n",
    "    bad_words_ids = get_bad_words_ids(bad_words, tokenizer)\n",
    "\n",
    "    # 토크나이징\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "\n",
    "    # 생성\n",
    "    output_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=64,\n",
    "        min_length=10,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True,\n",
    "        bad_words_ids=bad_words_ids\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f34e063a-0b12-4203-963e-cce6be20e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原原 마찬가지입니다原 민정수석 공개된다hankookilbohankookilbo 좋겠지만hankookilbo Stamfordhankookilbo 바르셀로나hankookilbo accidenthankookilbo 특활비hankookilbo크스hankookilbo 위메프hankookilbo 특수활동비hankookilbo ridehankookilbo 조치했다hankookilbo 많게는 Stamford Stamford 바르셀로나 바르셀로나 Stamford 코픽스 Stamfordgnrad Stamford 세계랭킹 StamfordNYT Stamford Bangkok Stamford Florence Stamford 일으키 Stamford Orange Stamford 윈도우 바르셀로나 코픽스 바르셀로나 분기별 Stamford 베이징올림픽 Stamford Munich Stamford Ankara Stamford 우리금융지주\n"
     ]
    }
   ],
   "source": [
    "context = [\n",
    "    \"A: 오늘 진짜 졸려 죽겠다\",\n",
    "    \"B: 어제 늦게 잤냐?\",\n",
    "    \"A: 응 밤새서 과제함\"\n",
    "]\n",
    "input_text = \"B: 그걸 밤샘이라고 하는 것도 웃기다ㅋㅋ\"\n",
    "\n",
    "print(rewrite(context, input_text, bad_words_file=\"bad_words.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d622f-7928-46e1-865c-98876cfcf781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3c2f4-34d4-40c3-850f-c7bfae80d0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933d890-f063-4919-9e72-373c4306ba4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
