{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d6593f9-1d2c-484b-a074-102d560e259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"KETI-AIR/ke-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "99ce89be-d3c6-4a68-a517-5cc066fc14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"C:/Users/asia/Desktop/íŒŒì´ë„í”„ë¡œì íŠ¸/03_ë°ì´í„°ì „ì²˜ë¦¬/ìˆœí™”í‘œí˜„ëª¨ë¸ ë°ì´í„°_ë¦¬ë¼ì´íŒ…ì™„ë£Œ.jsonl\"\n",
    "\n",
    "# ì¤„ ë‹¨ìœ„ JSON ë¡œë”©\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "955a0f88-ed62-41c8-b56f-d864a0e7475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] input_text:\n",
      "rephrase politely: A: ì˜¤ëŠ˜ ì§€ê°í•  ë»”í–ˆë‹¤ã…‹ã…‹ ë²„ìŠ¤ ì§„ì§œ ì•ˆ ì˜´ B: ë‚˜ë„ ê±°ì˜ ë›°ì–´ì„œ ì˜´; A: ìˆ˜í•™ìŒ¤ ì˜¤ëŠ˜ ë˜ ì§€ì˜¥ ì‹œê°„ì¼ ë“¯ B: ê·¸ ã…†ã…‚ ëŠ™ì€ì´ ëª©ì†Œë¦¬ë§Œ ë“¤ì–´ë„ í˜„íƒ€ ì˜´\n",
      "[0] target_text:\n",
      "B: ìˆ˜í•™ìŒ¤ ëª©ì†Œë¦¬ ë„ˆë¬´ ì§€ë£¨í•˜ê¸´ í•´\n",
      "============================================================\n",
      "[1] input_text:\n",
      "rephrase politely: A: ê¸‰ì‹ ì˜¤ëŠ˜ ë­ëƒ ? B: ì•„ë§ˆ ì œìœ¡ë³¶ìŒì´ì—ˆì„ê±¸ ? A: ê°œì¢‹ë‹¤ã…‹ã…‹ ë°¥ ë‘ ê³µê¸° ê° B: ë‹ˆ ê·¸ ë¼ì§€ê°™ì€ ëª¸ìœ¼ë¡œ ë˜ í¼ë¨¹ëƒã…‹ã…‹ ì—­ê²¹ë‹¤ ì§„ì§œ\n",
      "[1] target_text:\n",
      "B: ë¨¹ì„± ì§„ì§œ ì¢‹ë‹¤ ë„ˆ\n",
      "============================================================\n",
      "[2] input_text:\n",
      "rephrase politely: A: ê±” ì˜¤ëŠ˜ êµë³µì— ë‹¨ì¶” ë‹¤ í’€ê³  ì™”ë”ë¼ B: ê± ê¾¸ë¯¸ê³  ì‹¶ì—ˆë‚˜ë³´ì§€ A: ê·¼ë° ì„ ìƒë‹˜í•œí…Œ ì•ˆ ê±¸ë¦° ê²Œ ì‹ ê¸°í•¨ B: ì§€ ê¼´ì— íŒ¨ì…˜ë¦¬ë”ë˜ã…‹ã…‹ í† ë‚˜ì˜¨ë‹¤ ì§„ì‹¬\n",
      "[2] target_text:\n",
      "B: ê±” íŒ¨ì…˜ì— ê´€ì‹¬ ë§ì•„ë³´ì´ê¸´ í•´ ã…‹ã…‹\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    context = \"\\n\".join(example[\"context\"][:-1])  # ë§ˆì§€ë§‰ ë°œí™” ì œì™¸\n",
    "    input_text = f\"rephrase politely:\\n{context}\\n{example['context'][-1]}\"  # ë§ˆì§€ë§‰ ë°œí™”\n",
    "    input_text = clean_text(input_text)\n",
    "    target_text = clean_text(example[\"output\"])\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "\n",
    "processed_data = [preprocess(ex) for ex in data]\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"[{i}] input_text:\\n{processed_data[i]['input_text']}\")\n",
    "    print(f\"[{i}] target_text:\\n{processed_data[i]['target_text']}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e917ec2-acb1-45b4-ad0b-217f927818d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] input_text:\n",
      "rephrase politely: A: ì˜¤ëŠ˜ ì§€ê°í•  ë»”í–ˆë‹¤ã…‹ã…‹ ë²„ìŠ¤ ì§„ì§œ ì•ˆ ì˜´ B: ë‚˜ë„ ê±°ì˜ ë›°ì–´ì„œ ì˜´; A: ìˆ˜í•™ìŒ¤ ì˜¤ëŠ˜ ë˜ ì§€ì˜¥ ì‹œê°„ì¼ ë“¯ B: ê·¸ ã…†ã…‚ ëŠ™ì€ì´ ëª©ì†Œë¦¬ë§Œ ë“¤ì–´ë„ í˜„íƒ€ ì˜´ B: ê·¸ ã…†ã…‚ ëŠ™ì€ì´ ëª©ì†Œë¦¬ë§Œ ë“¤ì–´ë„ í˜„íƒ€ ì˜´\n",
      "[0] target_text:\n",
      "B: ìˆ˜í•™ìŒ¤ ëª©ì†Œë¦¬ ë„ˆë¬´ ì§€ë£¨í•˜ê¸´ í•´\n",
      "============================================================\n",
      "[1] input_text:\n",
      "rephrase politely: A: ê¸‰ì‹ ì˜¤ëŠ˜ ë­ëƒ ? B: ì•„ë§ˆ ì œìœ¡ë³¶ìŒì´ì—ˆì„ê±¸ ? A: ê°œì¢‹ë‹¤ã…‹ã…‹ ë°¥ ë‘ ê³µê¸° ê° B: ë‹ˆ ê·¸ ë¼ì§€ê°™ì€ ëª¸ìœ¼ë¡œ ë˜ í¼ë¨¹ëƒã…‹ã…‹ ì—­ê²¹ë‹¤ ì§„ì§œ B: ë‹ˆ ê·¸ ë¼ì§€ê°™ì€ ëª¸ìœ¼ë¡œ ë˜ í¼ë¨¹ëƒã…‹ã…‹ ì—­ê²¹ë‹¤ ì§„ì§œ\n",
      "[1] target_text:\n",
      "B: ë¨¹ì„± ì§„ì§œ ì¢‹ë‹¤ ë„ˆ\n",
      "============================================================\n",
      "[2] input_text:\n",
      "rephrase politely: A: ê±” ì˜¤ëŠ˜ êµë³µì— ë‹¨ì¶” ë‹¤ í’€ê³  ì™”ë”ë¼ B: ê± ê¾¸ë¯¸ê³  ì‹¶ì—ˆë‚˜ë³´ì§€ A: ê·¼ë° ì„ ìƒë‹˜í•œí…Œ ì•ˆ ê±¸ë¦° ê²Œ ì‹ ê¸°í•¨ B: ì§€ ê¼´ì— íŒ¨ì…˜ë¦¬ë”ë˜ã…‹ã…‹ í† ë‚˜ì˜¨ë‹¤ ì§„ì‹¬ B: ì§€ ê¼´ì— íŒ¨ì…˜ë¦¬ë”ë˜ã…‹ã…‹ í† ë‚˜ì˜¨ë‹¤ ì§„ì‹¬\n",
      "[2] target_text:\n",
      "B: ê±” íŒ¨ì…˜ì— ê´€ì‹¬ ë§ì•„ë³´ì´ê¸´ í•´ ã…‹ã…‹\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 1ë‹¨ê³„: ì „ì²´ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = re.sub(r\"(ã…‹){3,}\", \"ã…‹ã…‹\", text)\n",
    "    text = re.sub(r\"(ã…){3,}\", \"ã…ã…\", text)\n",
    "    text = re.sub(r\"(;){2,}\", \";\", text)\n",
    "    text = re.sub(r\"(\\.{2,})\", \"...\", text)\n",
    "    text = re.sub(r\"(!){2,}\", \"!!\", text)\n",
    "    text = re.sub(r\"(\\?){2,}\", \"??\", text)\n",
    "    text = re.sub(r\"([!?.,])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"([~â¤ğŸ’¢ğŸ’¥ğŸ’¬])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess(example):\n",
    "    context = \"\\n\".join(example[\"context\"])\n",
    "    input_text = f\"rephrase politely:\\n{context}\\n{example['input']}\"\n",
    "    input_text = clean_text(input_text)\n",
    "    target_text = clean_text(example[\"output\"])\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "\n",
    "# ì „ì²˜ë¦¬ ì ìš©\n",
    "processed_data = [preprocess(ex) for ex in data]\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "for i in range(3):\n",
    "    print(f\"[{i}] input_text:\\n{processed_data[i]['input_text']}\")\n",
    "    print(f\"[{i}] target_text:\\n{processed_data[i]['target_text']}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cda20ba4-aa67-4d40-9069-c2b4a4a4faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RewriteDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text = item[\"input_text\"]\n",
    "        target_text = item[\"target_text\"]\n",
    "\n",
    "        # í† í°í™”\n",
    "        model_inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        # squeezeë¡œ (1, N) â†’ (N) ì°¨ì› ì¶•ì†Œ\n",
    "        model_inputs = {k: v.squeeze() for k, v in model_inputs.items()}\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"].squeeze()\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "311159a3-39b1-4c7b-8605-a55d58e460a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ë°ì´í„° ë¶„í• \n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "train_dataset = RewriteDataset(train_data, tokenizer)\n",
    "test_dataset = RewriteDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b6fa70e-840b-42b5-b350-c11467d7cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./ke-t5-rewrite-small\",   # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "    learning_rate=2e-5,                   # í•™ìŠµë¥ \n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,                   # ì²´í¬í¬ì¸íŠ¸ ìµœëŒ€ 2ê°œ ì €ì¥\n",
    "    num_train_epochs=3,                   # ì—í­ ìˆ˜\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    generation_num_beams=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",          # ì´ê±° ê¼­ ë„£ì–´ì•¼ load_best_model_at_endê°€ ì‘ë™í•¨\n",
    "    save_strategy=\"steps\",               # ì €ì¥ê³¼ í‰ê°€ ì „ëµ ì¼ì¹˜ì‹œì¼œì•¼ ì—ëŸ¬ ì•ˆ ë‚¨\n",
    "    save_steps=500,                        # 50 stepë§ˆë‹¤ ì €ì¥\n",
    "    eval_steps=500,                        # 50 stepë§ˆë‹¤ í‰ê°€\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=100\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82460d82-d14f-4253-92a3-8f1df68fedd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='4050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/4050 07:32 < 53:40, 1.10 it/s, Epoch 0.37/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 05:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì˜ˆì¸¡: ì•„ì˜ˆ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤åŸ ëˆ„êµ°ê°€ramprampramprampramp ì°¨ì§ˆ ì°¨ì§ˆ ë¬¼ì  í‰ê· ì ìˆ˜ í‰ê· ì ìˆ˜rampramprampramprampramprampramp í‰ê· ì ìˆ˜rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp í‰ê· ì ìˆ˜\n",
      "ì •ë‹µ: B: ì˜›ë‚  ë§íˆ¬ ê°™ì•„ì„œ ì˜¤íˆë ¤ ì •ê²¹ì§€ ì•Šì•„?\n",
      "---\n",
      "\n",
      "ì˜ˆì¸¡: ì•„ì˜ˆ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤åŸ ê¹€ì‹ íšŒrampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp\n",
      "ì •ë‹µ: B: ì—´ì‹¬íˆ ì¤€ë¹„í–ˆìœ¼ë‹ˆ ì¢‹ì€ ê²°ê³¼ ë‚˜ì˜¬ ê±°ì•¼!\n",
      "---\n",
      "\n",
      "ì˜ˆì¸¡: ì•„ì˜ˆ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤ ë´ì£¼ ëˆ„êµ°ê°€ ë§ˆì•„ë¼ ë¬¼ì  í‰ê· ì ìˆ˜ í‰ê· ì ìˆ˜ì°½ì—…íŒ€rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp\n",
      "ì •ë‹µ: B: ì¢€ íì‡„ì ì¸ ê³³ë„ ìˆìœ¼ë‹ˆ ë¶„ìœ„ê¸° í™•ì¸í•´ë³´ëŠ” ê²Œ ì¢‹ê² ë‹¤\n",
      "---\n",
      "\n",
      "ì˜ˆì¸¡: ì•„ì˜ˆ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤ ë§ˆì°¬ê°€ì§€ë‹¤ì°½ì—…íŒ€ elsewhere í‰ê· ì ìˆ˜ í‰ê· ì ìˆ˜ramprampramprampramp í‰ê· ì ìˆ˜ramprampramp í‰ê· ì ìˆ˜ramprampramp í‰ê· ì ìˆ˜rampramp í‰ê· ì ìˆ˜rampramp í‰ê· ì ìˆ˜rampramp í‰ê· ì ìˆ˜rampramp í‰ê· ì ìˆ˜rampramp í‰ê· ì ìˆ˜rampramp í‰ê· ì ìˆ˜ramp í‰ê· ì ìˆ˜rampramp í‰ê· ì ìˆ˜ramp í‰ê· ì ìˆ˜ramp í‰ê· ì ìˆ˜rampramprampramprampramprampramp í‰ê· ì ìˆ˜rampramp í‰ê· ì ìˆ˜ í‰ê· ì ìˆ˜ í‰ê· ì ìˆ˜\n",
      "ì •ë‹µ: B: ê·¸ë˜, ì„œë¡œ ì˜ˆë¯¼í–ˆì„ ìˆ˜ë„ ìˆì§€. ë‚˜ë„ ì˜ í’€ê³  ì‹¶ì–´.\n",
      "---\n",
      "\n",
      "ì˜ˆì¸¡: ì•„ì˜ˆ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤åŸ ëˆ„êµ°ê°€ ë¬¼ì  í‰ê· ì ìˆ˜rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp\n",
      "ì •ë‹µ: B: ë°˜ì°¬ ìª½ì€ ì¢€ ì‹¬ì‹¬í–ˆì–´. ìµìˆ™í•œ ë§›ì´ê¸´ í–ˆì§€\n",
      "---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value('string'), 'references': List(Value('string'))}\nFeature option 1: {'predictions': Value('string'), 'references': Value('string')},\nInput predictions: ['ì•„ì˜ˆ', 'ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤', 'ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤åŸ', ..., 'í‰ê· ì ìˆ˜rampramprampramprampramprampramp', 'í‰ê· ì ìˆ˜rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp', 'í‰ê· ì ìˆ˜'],\nInput references: [['B:', 'ì˜›ë‚ ', 'ë§íˆ¬', 'ê°™ì•„ì„œ', 'ì˜¤íˆë ¤', 'ì •ê²¹ì§€', 'ì•Šì•„?']]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ke-t5-rewrite-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ke-t5-rewrite-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1538\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1539\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1540\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1541\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1542\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1914\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1911\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1912\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1914\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1916\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2268\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys_for_eval)\n\u001b[0;32m   2269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:166\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys, metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3019\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3016\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3018\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3019\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[0;32m   3020\u001b[0m     eval_dataloader,\n\u001b[0;32m   3021\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3022\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[0;32m   3023\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[0;32m   3024\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3025\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[0;32m   3026\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[0;32m   3027\u001b[0m )\n\u001b[0;32m   3029\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3310\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3306\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3307\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3308\u001b[0m         )\n\u001b[0;32m   3309\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3310\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels))\n\u001b[0;32m   3311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3312\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[54], line 23\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_preds)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# BLEUëŠ” í† í°í™” í•„ìš”\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m bleu_metric\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m     24\u001b[0m     predictions\u001b[38;5;241m=\u001b[39m[pred\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m decoded_preds],\n\u001b[0;32m     25\u001b[0m     references\u001b[38;5;241m=\u001b[39m[[label\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m decoded_labels]\n\u001b[0;32m     26\u001b[0m )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# ROUGE-L ê³„ì‚°\u001b[39;00m\n\u001b[0;32m     29\u001b[0m rouge_result \u001b[38;5;241m=\u001b[39m rouge_metric\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m     30\u001b[0m     predictions\u001b[38;5;241m=\u001b[39mdecoded_preds,\n\u001b[0;32m     31\u001b[0m     references\u001b[38;5;241m=\u001b[39mdecoded_labels,\n\u001b[0;32m     32\u001b[0m     use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:514\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m batch \u001b[38;5;241m=\u001b[39m {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_batch(batch)\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:596\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    595\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:616\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_example\u001b[1;34m(self, example)\u001b[0m\n\u001b[0;32m    609\u001b[0m feature_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature option \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures)])\n\u001b[0;32m    610\u001b[0m error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature_strings\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    615\u001b[0m )\n\u001b[1;32m--> 616\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value('string'), 'references': List(Value('string'))}\nFeature option 1: {'predictions': Value('string'), 'references': Value('string')},\nInput predictions: ['ì•„ì˜ˆ', 'ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤', 'ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤åŸ', ..., 'í‰ê· ì ìˆ˜rampramprampramprampramprampramp', 'í‰ê· ì ìˆ˜rampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramprampramp', 'í‰ê· ì ìˆ˜'],\nInput references: [['B:', 'ì˜›ë‚ ', 'ë§íˆ¬', 'ê°™ì•„ì„œ', 'ì˜¤íˆë ¤', 'ì •ê²¹ì§€', 'ì•Šì•„?']]"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./ke-t5-rewrite-small\")\n",
    "tokenizer.save_pretrained(\"./ke-t5-rewrite-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "640617d7-7836-448a-93fb-60250bc02dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def load_model_and_tokenizer(model_path=\"./ke-t5-rewrite-small\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_bad_words(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def get_bad_words_ids(bad_words, tokenizer):\n",
    "    return tokenizer(bad_words, add_special_tokens=False).input_ids\n",
    "\n",
    "def rewrite(context, input_text, model, tokenizer, bad_words_file=\"badwords.txt\"):\n",
    "    # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    context_text = \"\\n\".join(context)\n",
    "    prompt = f\"rephrase politely:\\n{context_text}\\n{input_text}\"\n",
    "\n",
    "    # ë¹„ì†ì–´ ë¡œë”© ë° ID ë³€í™˜\n",
    "    bad_words = load_bad_words(bad_words_file)\n",
    "    bad_words_ids = get_bad_words_ids(bad_words, tokenizer)\n",
    "\n",
    "    # í† í¬ë‚˜ì´ì§•\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "\n",
    "    # ìƒì„±\n",
    "    output_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=64,\n",
    "        min_length=10,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True,\n",
    "        bad_words_ids=bad_words_ids\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f34e063a-0b12-4203-963e-cce6be20e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸåŸ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤åŸ ë¯¼ì •ìˆ˜ì„ ê³µê°œëœë‹¤hankookilbohankookilbo ì¢‹ê² ì§€ë§Œhankookilbo Stamfordhankookilbo ë°”ë¥´ì…€ë¡œë‚˜hankookilbo accidenthankookilbo íŠ¹í™œë¹„hankookilboí¬ìŠ¤hankookilbo ìœ„ë©”í”„hankookilbo íŠ¹ìˆ˜í™œë™ë¹„hankookilbo ridehankookilbo ì¡°ì¹˜í–ˆë‹¤hankookilbo ë§ê²ŒëŠ” Stamford Stamford ë°”ë¥´ì…€ë¡œë‚˜ ë°”ë¥´ì…€ë¡œë‚˜ Stamford ì½”í”½ìŠ¤ Stamfordgnrad Stamford ì„¸ê³„ë­í‚¹ StamfordNYT Stamford Bangkok Stamford Florence Stamford ì¼ìœ¼í‚¤ Stamford Orange Stamford ìœˆë„ìš° ë°”ë¥´ì…€ë¡œë‚˜ ì½”í”½ìŠ¤ ë°”ë¥´ì…€ë¡œë‚˜ ë¶„ê¸°ë³„ Stamford ë² ì´ì§•ì˜¬ë¦¼í”½ Stamford Munich Stamford Ankara Stamford ìš°ë¦¬ê¸ˆìœµì§€ì£¼\n"
     ]
    }
   ],
   "source": [
    "context = [\n",
    "    \"A: ì˜¤ëŠ˜ ì§„ì§œ ì¡¸ë ¤ ì£½ê² ë‹¤\",\n",
    "    \"B: ì–´ì œ ëŠ¦ê²Œ ì¤ëƒ?\",\n",
    "    \"A: ì‘ ë°¤ìƒˆì„œ ê³¼ì œí•¨\"\n",
    "]\n",
    "input_text = \"B: ê·¸ê±¸ ë°¤ìƒ˜ì´ë¼ê³  í•˜ëŠ” ê²ƒë„ ì›ƒê¸°ë‹¤ã…‹ã…‹\"\n",
    "\n",
    "print(rewrite(context, input_text, bad_words_file=\"bad_words.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d622f-7928-46e1-865c-98876cfcf781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3c2f4-34d4-40c3-850f-c7bfae80d0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933d890-f063-4919-9e72-373c4306ba4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
